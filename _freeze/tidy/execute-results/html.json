{
  "hash": "b1f840a43b72aa3eb871dfa75fb71a92",
  "result": {
    "markdown": "---\ntitle: \"Tidy data in R\"\neditor: visual\n---\n\n::: {.cell}\n\n:::\n\n\nThe [Tidyverse](http://tidyverse.org) is a collection of R packages that adhere to the *tidy data* principles of data analysis and graphing. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the last few years other individuals have added some packages to the collective, which has significantly expanded our data analytical capabilities through improved ease of use and efficiency. The Tidyverse packages can be loaded collectively by calling the **`tidyverse`** package, as we have seen throughout this workshop. The packages making up the Tidyverse are shown in Figure \\@ref(fig:tidyverse).\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\nAs we may see in the following figure (Figure \\@ref(fig:tidy)), the tidying of ones data should be the second step in any workflow, after the loading of the data.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nBut what exactly are **tidy data**? It is not just a a buzz word, there is a real definition. In three parts, to be exact. Taken from Hadley Wickham's [R for Data Science](http://r4ds.had.co.nz/workflow-basics.html):\n\n> 1.  Each variable must have its own column.  \n2.  Each observation must have its own row.  \n3.  Each value must have its own cell.  \n\nThis is represented graphically in figure \\@ref(fig:tidy-structure). One will generally satisfy these three rules effortlessly simply by never putting more than one dataset in a file, and never putting more (or less) than one variable in the same column. We will go over this several more times today so do not fret if those guidelines are not immediately clear.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nIn order to illustrate the meaning of this three part definition we are going to learn how to manipulate a non-tidy dataset into a tidy one. To do so we will need to learn a few new, very useful functions. Let's load our demo dataset to get started. This snippet from the SACTN dataset contains only data for 2008 - 2009 for three time series, with some notable (untidy) changes. The purpose of the following exercises is not only to show how to tidy data, but to also illustrate that these steps may be done more quickly in R than excel, allowing for ones raw data to remain exactly how they were collected, with all of the manipulations performed on them documented in an R script. This is a centrally important part of reproducible research.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/SACTN_mangled.RData\")\n```\n:::\n\n\nWith our data loaded let's now have a peek at them. We will first see that we have loaded not one, but five different objects into our environment pane in the top left our RStudio window. These all contain the exact same data in different states of disrepair. As one may guess, some of these datasets will be easier to use than others.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN1\nSACTN2\nSACTN3\n\n# Spread across two dataframes\nSACTN4a\nSACTN4b\n```\n:::\n\n\nWe start off by looking at `SACTN1`. If these data look just like all of the other SACTN data we've used thus far that's because they are. These are how tidy data should look. No surprises. In fact, because these data are already tidy it is very straightforward to use them for whatever purposes we may want. Making a time series plot, for example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = SACTN1, aes(x = date, y = temp)) +\n  geom_line(aes(colour = site, group = paste0(site, src))) +\n  labs(x = \"\", y = \"Temperature (Â°C)\", colour = \"Site\") +\n  theme_bw()\n```\n:::\n\n\n> **`%>%`**  \nRemember that this funny series of symbols is the pipe operator. It combines consequetive rows of code together so that they run as though they were one 'chunk'. We will be seeing this symbol a lot today. The keyboard shortcut for `%>%` is **ctrl shift m**. \n\n## Gathering and spreading\n\nBefore *tidy* became the adjective used to describe neatly formatted data, people used to say *long*. This is because well organised dataframes tend to always be longer than they are wide (with the exception of species assemblage data). The opposite of *long* data are *wide* data. If one ever finds a dataset that is wider than it is long then this is probably because the person that created them saved one variable across many columns. \n\nAs we sit here and read through these examples it may seem odd that so much effort is being spent on something so straightforward as tidy data. Surely this is too obvious to devote an entire day of work to it? Unfortunately not. As we go out into the wild world of 'real life data', we tend to find that very few datasets (especially those collected by hand) are tidy. Rather they are plagued by any number of issues. The first step then for tidying up the data are to have a look at them and discern what are the observations that were made/recorded, and what are the variables within those observations. Let's have a look now at `SACTN2` for an example of what *wide* data looks like, and how to fix it.\n\n### Gathering\n\nIn `SACTN2` we can see that the `src` column has been removed and that the temperatures are placed in columns that denote the collecting source. This may at first seem like a reasonable way to organise these data, but it is not tidy because the collecting source is one variable, and so should not take up more than one column. We need to `gather()` these source columns back together. We do this by telling `gather()` what the names of the columns are we want to squish together. We then tell it the name of the 'key' column. This is the column that will contain all of the old column names we are gathering. In this case we will call it 'date'. The last piece of this puzzle is the 'value' column. This is where we decide what the name of the column will be for values we are gathering up. In this case we will name it 'temp', because we are gathering up the temperature values that were incorrectly spread out by month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN2_tidy <- SACTN2 %>%\n  gather(DEA, KZNSB, SAWS, key = \"src\", value = \"temp\")\n```\n:::\n\n\n### Spreading\n\nShould ones data be too long, meaning when individual observations are spread across multiple rows, we will need to use `spread()` to rectify the situation. This is generally the case when we have two or more variables stored within the same column, as we may see in `SACTN3`. This is not terribly common as it would require someone to put quite a bit of time into making a dataframe this way. But never say never. To spread data we first tell R what the name of the column is that contains more than one variable, in this case the 'var' column. We then tell R what the name of the column is that contains the values that need to be spread, in this case the 'val' column. Notice here that these two column names are not given in inverted commas. This is because with gather, we were creating two new columns, and so we fed them to R as character vectors (i.e. inside of inverted commas). With the spread function we are naming existing columns and so we give them to R without inverted commas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN3_tidy <- SACTN3 %>% \n  spread(key = var, value = val)\n```\n:::\n\n\n## Separating and uniting\n\nWe've now covered how to make our dataframes longer or wider depending on their tidiness. Now we will look at how to manage our columns when they contain more (or less) than one variable, but the overall dataframe does not need to be made wider or longer. This is generally the case when one has a column with two variables, or two or more variables are spread out across multiple columns, but there is still only one observation per row. Let's see some examples to make this more clear.\n\n### Separate\n\nIf we look at `SACTN4a` we see that we no longer have a `site` and `src` column. Rather these have been replaced by an `index` column. This is an efficient way to store these data, but it is not tidy because the site and source of each observation are separate variables. To re-create our `site` and `src` columns we must `separate()` the `index` column. First we give R the name of the column we want to separate, in this case `index`. Next we must say what the names of the new columns will be. Remember that because we are creating new column names we feed these into R within inverted commas. Lastly we should tell R how to separate the `index` column. If we look at the data we may see that the values we want to split up are separated with '/ ', so that is what we give to R. Often times the `separate()` function is able to guess correctly, but it is better to be explicit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN4a_tidy <- SACTN4a %>% \n  separate(col = index, into = c(\"site\", \"src\"), sep = \"/ \")\n```\n:::\n\n\n### Unite\n\nIt is not uncommon that field/lab instruments split values across multiple columns while they are making recordings. I see this most often with date values. Often the year, month, and day values are given in different columns. There are uses for the data in this way, though it is not terribly tidy. We usually want the date of any observation to be shown in just one column. If we look at `SACTN4b` we will see that there is a `year`, `month`, and `day` column. To `unite()` them we must first tell R what we want the united column to be labelled, in this case we will use 'date'. We then list the columns to be united, her this is `year`, `month`, and `day`. Lastly we must specify if we want the united values to have a separator between them. The standard separator for date values is '-'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN4b_tidy <- SACTN4b %>% \n  unite(year, month, day, col = \"date\", sep = \"-\")\n```\n:::\n\n\n## Joining\n\nWe will end this session with the concept of joining two different dataframes. Remember that one of the rules of tidy data is that only one complete dataset is saved per dataframe. This rule then is violated not only when additional data are stored where they don't belong, but also when necessary data are saved elsewhere. If we look back at `SACTN4a` and `SACTN4b` we will see that they are each missing different columns. Were we to **join** these dataframes together they would complete each other. The `tidyverse` provides us with several methods of doing this, but we will demonstrate here only the most common technique. The function `left_join()` is so named because it joins two or more dataframes together based on the matching of columns from the left to the right. It combines values together where it sees that they match up, and adds new rows and columns where they do not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN4_tidy <- left_join(SACTN4a_tidy, SACTN4b_tidy)\n```\n:::\n\n\nAs we see above, if we let `left_join()` do it's thing it will make a plan for us and find the common columns and match up the values and observations for us best it thinks. It then returns a message letting us know what it's done. That is a pleasant convenience, but we most likely want to exert more control over this process than that. In order to specify the columns to be used for joining we must add one more argument to `left_join()`. The `by` argument must be fed a list of column names in inverted commas if we want to specify how to join our dataframes. Not that when we run this it does not produce a message as we have provided enough explicit information that the machine is no longer needing to think for itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN4_tidy <- left_join(SACTN4a_tidy, SACTN4b_tidy, by = c(\"site\", \"src\", \"date\"))\n```\n:::\n\n\n\n## But why though?\n\nAt this point one may be wondering what the point of all of this is. Sure it's all well and good to see how to tidy one's data in R, but couldn't this be done more quickly and easily in Excel? Perhaps, yes, with a small dataset. But remember, (for many) the main reason we are learning R is to ensure that we are performing reproducible research. This means that every step in our workflow must be documented. And we accomplish this by writing R scripts.\n\n\nOn Day 1 already we walked ourselves through a tidy workflow. We saw how to import data, how to manipulate it, run a quick analysis or two, and create figures. In the previous session we filled in the missing piece of the workflow by also learning how to tidy up our data within R. For the remainder of today we will be revisiting the 'transform' portion of the tidy workflow. In this session we are going to go into more depth on what we learned in Day 1, and in the last session we will learn some new tricks. Over these two sessions we will also become more comfortable with the *pipe* command `%>%`, while practising writing tidy code.\n\nThere are five primary data transformation functions that we will focus on here:  \n\n* Arrange observations (rows)  with `arrange()`  \n* Filter observations (rows) with `filter()`  \n* Select variables (columns) with`select()`  \n* Create new variables (columns) with `mutate()`  \n* Summarise variables (columns) with `summarise()`  \n\nWe will use the full South African Coastal Temperature Network dataset for these exercises. Before we begin however we will need to cover two new concepts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load the data from a .RData file\nload(\"data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN <- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)\n```\n:::\n\n\n## Comparison operators\n\nThe assignment operator (`<-`) is a symbol that we use to assign some bit of code to an object in our environment. Likewise, comparison operators are symbols we use to compare different objects. This is how we tell R how to decide to do many different things. We will see these symbols often out in the 'real world' so let's spend a moment now getting to know them better. Most of these should be very familiar to us:  \n\n* Greater than: `>`  \n* Greater than or equal to: `>=`  \n* Less than: `<`  \n* Less than or equal to: `<=`  \n* Equal to: `==`  \n* Not equal to: `!=`  \n\nIt is important here to note that `==` is for comparisons and `=` is for maths. They are **not** interchangeable, as we may see in the following code chunk. This is one of the more common mistakes one makes when writing code. Luckily the error message this creates should provide us with the clues we need to figure out that we have made this specific mistake.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site = \"Amanzimtoti\")\n```\n:::\n\n\n## Logical operators\n\nComparison operators are used to make direct comparisons between specific things, but logical operators are used more broadly when making logical arguments. Logic is central to most computing so it is worth taking the time to cover these symbols explicitly here. R makes use of the same *Boolean logic* symbols as many other platforms, including Google, so some (or all) of these will likely be familiar. We will generally only use three:  \n\n* and: `&`  \n* or: `|`  \n* not: `!`  \n  \nWhen writing a line of tidy code we tend to use these logical operator to combine two or more arguments that use comparison operators. For example, the following code chunk uses the `filter()` function to find all temperatures recorded at Pollock Beach during December **OR** January. Don't worry if the following line of code is difficult to piece out, but make sure you can locate which symbols are comparison operators and which are logical operators. Please note that for purposes of brevity all of the outputs in this section are limited to ten lines, but when one runs these code chunks on ones own computer they will be much longer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Pollock Beach\", month(date) == 12 | month(date) == 1)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nWe will look at the interplay between comparison and logical operators in more depth in the following session after we have reacquainted ourselves with the main transformation functions we need to know.\n\n## Arrange observations (rows)  with `arrange()`\n\nFirst up in our greatest hits reunion tour is the function `arrange()`. This very simply arranges the observations (rows) in a dataframe based on the variables (columns) it is given. If we are concerned with ties in the ordering of our data we provide additional columns to `arrange()`. The importance of the columns for arranging the rows is given in order from left to right.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  arrange(depth, temp)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nIf we would rather arrange our data in descending order, as is perhaps more often the case, we simply wrap the column name we are arranging by with the `desc()` function as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  arrange(desc(temp))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nIt must also be noted that when arranging data in this way, any rows with `NA` values will be sent to the bottom of the dataframe. This is not always ideal and so must be kept in mind.\n\n## Filter observations (rows) with `filter()`\n\nWhen simply arranging data is not enough, and we need to remove rows of data we do not want, `filter()` is the tool to use. For example, we can select all monthly temperatures recorded at the `site` Humewood during the `year` 1990 with the following code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Humewood\", year(date) == 1990)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nRemember to use the assignment operator (`<-`, keyboard shortcut **alt -**) if one wants to create an object in the environment with the new results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhumewood_90s <- SACTN %>% \n  filter(site == \"Humewood\", year(date) %in% seq(1990, 1999, 1))\n```\n:::\n\n\nIt must be mentioned that `filter()` also automatically removes any rows in the filtering column that contain `NA` values. Should one want to keep rows that contain missing values, insert the `is.na()` function into the line of code in question. To illustrate this let's filter the temperatures for the Port Nolloth data collected by the DEA that were at or below 11Â°C OR were missing values. We'll put each argument on a separate line to help keep things clear. Note how R automatically indents the last line in this chunk to help remind us that they are in fact part of the same argument. Also note how I have put the last bracket at the end of this argument on it's own line. This is not required, but I like to do so as it is a very common mistake to forget the last bracket.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Port Nolloth\", # First give the site to filter\n         src == \"DEA\", # Then specify the source\n         temp <= 11 | # Temperatures at or below 11Â°C OR\n           is.na(temp) # Include missing values\n         )\n```\n:::\n\n\n## Select variables (columns) with`select()`\n\nWhen one loads a dataset that contains more columns than will be useful or required it is preferable to shave off the excess. We do this with the `select()` function. In the following four examples we are going to remove the `depth` and `type` columns. There are many ways to do this and none are technically better or faster. So it is up to the user to find a favourite technique.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select columns individually by name\nSACTN %>% \n  select(site, src, date, temp)\n\n# Select all columns between site and temp like a sequence\nSACTN %>% \n  select(site:temp)\n\n# Select all columns except those stated individually\nSACTN %>% \n  select(-date, -depth)\n\n# Select all columns except those within a given sequence\n  # Note that the '-' goes outside of a new set of brackets\n  # that are wrapped around the sequence of columns to remove\nSACTN %>% \n  select(-(date:depth))\n```\n:::\n\n\nWe may also use `select()` to reorder the columns in a dataframe. In this case the inclusion of the `everything()` function may be a useful shortcut as illustrated below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change up order by specifying individual columns\nSACTN %>% \n  select(temp, src, date, site)\n\n# Use the everything function to grab all columns \n# not already specified\nSACTN %>% \n  select(type, src, everything())\n\n# Or go bananas and use all of the rules at once\n  # Remember, when dealing with tidy data,\n  # everything may be interchanged\nSACTN %>% \n  select(temp:type, everything(), -src)\n```\n:::\n\n\n## Create new variables (columns) with `mutate()`\n\nWhen one is performing data analysis/statistics in R this is likely because it is necessary to create some new values that did not exist in the raw data. The previous three functions we looked at (`arrange()`, `filter()`, `select()`) will prepare us to create new data, but do not do so themselves. This is when we need to use `mutate()`. We must however be very mindful that `mutate()` is only useful if we want to create new variables (columns) that are a function of one or more *existing* columns. This means that any column we create with `mutate()` will always have the same number of rows as the dataframe we are working with. In order to create a new column we must first tell R what the name of the column will be, in this case let's create a column named `kelvin`. The second step is to then tell R what to put in the new column. AS you may have guessed, we are going to convert the `temp` column into Kelvin (Â°K) by adding 273.15 to every row.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  mutate(kelvin = temp + 273.15))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nThis is a very basic example and `mutate()` is capable of much more than simple addition. We will get into some more exciting examples during the next session.\n\n## Summarise variables (columns) with `summarise()`\n\nFinally this brings us to the last tool for this section. To create new columns we use `mutate()`, but to calculate any sort of summary/statistic from a column that will return fewer rows than the dataframe has we will use `summarise()`. This makes `summarise()` much more powerful than the other functions in this section, but because it is able to do more, it can also be more unpredictable, making it's use potentially more challenging. We will almost always end op using this function in our work flows however so it behoves us to become well acquainted with it. The following chunk very simply calculates the overall mean temperature for the entire SACTN.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE))\n```\n:::\n\n\nNote how the above chunk created a new dataframe. This is done because it cannot add this one result to the previous dataframe due to the mismatch in the number of rows. Were we to want to create additional columns with other summaries we may do so within the same `summarise()` function. These multiple summaries are displayed on individual lines in the following chunk to help keep things clear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE),\n            sd_temp = sd(temp, na.rm = TRUE),\n            min_temp = min(temp, na.rm = TRUE),\n            max_temp = max(temp, na.rm = TRUE)\n            )\n```\n:::\n\n\nCreating summaries of the *entire* SACTN dataset in this way is not appropriate as we should not be combining time series from such different parts of the coast. In order to calculate summaries within variables we will need to learn how to use `group_by()`, which in turn will first require us to learn how to chain multiple functions together within a pipe (`%>%`). That is how we will begin the next session for today. Finishing with several tips on how to make our data the tidiest that it may be.\n\n\nIn the previous session we covered the five main transformation functions one would use in a typical tidy workflow. But to really unlock their power we need to learn how to use them with `group_by()`. This is how we may calculate statistics based on the different grouping variables within our data, such as sites or species or soil types, for example. Let's begin by loading the **`tidyverse`** package and the SACTN data if we haven't already.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# load the data from a .RData file\nload(\"data/SACTNmonthly_v4.0.RData\")\n\n# Copy the data as a dataframe with a shorter name\nSACTN <- SACTNmonthly_v4.0\n\n# Remove the original\nrm(SACTNmonthly_v4.0)\n```\n:::\n\n\n## Group observations (rows) by variables (columns) with `group_by()`  \n\nWith the SACTN dataset loaded we will now look at the effect that `group_by` has on other `tidyverse` functions. First we shall create a new object called `SACTN_depth`. If we look at this object in the RStudio GUI it will not appear to be any different from `SACTN`. We may think of `group_by()` as working behind the scenes in order to better support the five main functions. We see that `group_by()` is working as intended when we create summaries from the `SACTN_depth` dataframe. Remember that one does not need to put the last bracket own it's own line. I like to do so in order to reduce the chances that I will forget to type it after the final argument within the last function in the code chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Group by depth\nSACTN_depth <- SACTN %>% \n  group_by(depth)\n\n# Calculate mean temperature by depth\nSACTN_depth_mean <- SACTN_depth %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE),\n  count = n()\n)\n\n# Visualise the results\nSACTN_depth_mean\n```\n:::\n\n\nLet's visualise our newly created summary dataframe and see what we get.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Why does the relationship between depth and temperature look so odd?\nggplot(data = SACTN_depth_mean, mapping = aes(x = depth, y = mean_temp)) +\n  geom_point(aes(size = count), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n```\n:::\n\n\n### Grouping by multiple variables\n\nAs one may have guessed by now, grouping is not confined to a single column. One may use any number of columns to perform elaborate grouping measures. Let's look at some ways of doing this with the SACTN data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create groupings based on temperatures and depth\nSACTN_temp_group <- SACTN %>% \n  group_by(round(temp), depth)\n\n# Create groupings based on source and date\nSACTN_src_group <- SACTN %>% \n  group_by(src, date)\n\n# Create groupings based on date and depth\nSACTN_date_group <- SACTN %>% \n  group_by(date, depth)\n```\n:::\n\n\nNow that we've created some grouped dataframes, let's think of some ways to summarise these data.\n\n### Ungrouping\n\nOnce we level up our **`tidyverse`** skills we will routinely be grouping variables while calculating statistics. This then poses the problem of losing track of which dataframes are grouped and which aren't. Happily, to remove any grouping we just use `ungroup()`. No arguments required, just the empty function by itself. Too easy. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN_ungroup <- SACTN_date_group %>% \n  ungroup()\n```\n:::\n\n\n## Chain functions with the pipe (`%>%`)\n\nThis now brings us to the last major concept we need to become confident with for this workshop. Everything we have learned thus far builds on everything else in a modular way, with most concepts and steps therein being interchangeable. The pipe takes all of the things we have learned and takes them to the next level. And the best part about it is that it requires us to learn nothing new. We've been doing it all along, perhaps without even realising it. Let's see what I mean. The following chunk does in one command what the first chunk in the previous section took two steps. This is not only faster, but saves us from having to create intermediate dataframes that only slow down our computer and clog up our environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN_depth_mean_2 <- SACTN %>% # Choose a base dataframe\n  group_by(depth) %>% # Group by thedepth column\n  summarise(mean_temp = mean(temp, na.rm = TRUE), # Calculate means\n  count = n() # Count observations\n) # Safety measure\n```\n:::\n\n\nTake a moment and compare the `SACTN_depth_mean_2` object that we've just created against the `SACTN_depth_mean` object we created at the beginning of this session. Same same.\n\nNot only does this keep our workflow tidier, it also makes it easier to read for ourselves, our colleagues, and most importantly, our future selves. When we look at the previous code chunk we can think of it as a paragraph in a research report, with each line a sentence. If I were to interpret this chunk of code in plain English I think it would sound something like this: \n  \n> In order to create the `SACTN_depth_mean_2` dataframe I first started by taking the original SACTN data.  \nI then grouped the data into different depth categories.  \nAfter this I calculated the mean temperature for each depth category, as well as counting the number of observations within each depth group.\n\nJust like paragraphs in a human language may vary in length, so too may code chunks. There really is no limit. This is not to say that it is encouraged to attempt to reproduce a code chunk of comparable length to anything Marcel Proust would have written. It is helpful to break things up into pieces of a certain size. What that size is though is open to the discretion of the person writing the code. It is up to you to find out for yourself what works best for you.\n\n## Group all the functions!\n\nWe've played around quite a bit with grouping and summarising, but that's not all we can do. We can use `group_by()` very nicely with `filter()` and `mutate()` as well. Not so much with `arrange()` and `select()` as these are designed to work on the entire dataframe at once, without any subsetting.\n\nWe can do some rather imaginative things when we combine all of these tools together. In fact, we should be able to accomplish almost any task we can think of. For example, what if we wanted to create a new object that was a subset of only the sites in the `SACTN` that had at least 30 years (360 months) of data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN_30_years <- SACTN %>%\n  group_by(site, src) %>%\n  filter(n() > 360)\n```\n:::\n\n\nOr what if we wanted to calculate anomaly data for each site?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN_anom <- SACTN %>%\n  group_by(site, src) %>% \n  mutate(anom = temp - mean(temp, na.rm = T)) %>% \n  select(site:date, anom, depth, type) %>% \n  ungroup()\n```\n:::\n\n\nNow, let's select only two sites and calculate their mean and standard deviations. Note how whichever columns we give to `group_by()` will be carried over into the new dataframe created by `summarise()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Paternoster\" | site == \"Oudekraal\") %>%\n  group_by(site, src) %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE), \n            sd_temp = sd(temp, na.rm = TRUE))\n```\n:::\n\n\n## Going deeper\n\nWe learned in the previous session that one should avoid using comparison operators to compare logical arguments as this tends to not produce the results one would expect. Below we see what happens when we try to repeat the code chunk above, but using a logical operator within a comparison operator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Paternoster\" | \"Oudekraal\") %>% # This line has been changed/shortened\n  group_by(site, src) %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE), \n            sd_temp = sd(temp, na.rm = TRUE))\n```\n:::\n\n\nOh no, we broke it! This is a common error while learning to write code so do try to keep this rule in mind as it can cause a lot of headaches. An easy way to spot this problem is if ones line of code has more logical operators than comparison operators you're probably going to have a bad time. This is doubly unfortunate as we would need to write less code if this were not so. Happily, there is a shortcut for just this problem, `%in%`. Whenever we want to use operators to filter by more than two things, it is most convenient to create an object that contains the names or numbers that we want to filter by. We then replace our comparison and logical operators with that one simple symbol (`%in%`). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First create a character vector containing the desired sites\nselected_sites <- c(\"Paternoster\", \"Oudekraal\", \"Muizenberg\", \"Humewood\")\n\n# Then calculate the statistics\nSACTN %>% \n  filter(site %in% selected_sites) %>%\n  group_by(site, src) %>% \n  summarise(mean_temp = mean(temp, na.rm = TRUE), \n            sd_temp = sd(temp, na.rm = TRUE))\n```\n:::\n\n\nThe `%in%` operator can be a very useful shortcut, but sometime we cannot avoid the comparison and logical operator dance. For example, if one wanted to find temperatures at Port Nolloth that were over 10Â°C but under 15Â°C you could use either of the following two filters. Remember that whenever we see a `,` in the filter function it is the same as the `&` logical operator. Of the two different techniques shown below, I would be more inclined to use the first one. The fewer symbols we use to write our code the better. Both for readability and error reduction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  filter(site == \"Port Nolloth\", temp > 10, temp < 15)\n\nSACTN %>% \n  filter(site == \"Port Nolloth\", !(temp <= 10 | temp  >= 15))\n```\n:::\n\n\nAs one may imagine, performing intricate logical arguments like this may get out of hand rather quickly. It is advisable to save intermediate steps in a complex workflow to avoid too much heartache. Where exactly these 'fire breaks' should be made is up to the person writing the code.\n\n## Pipe into **`ggplot2`**\n\nIt is also possible to combine piped code chunks and **`ggplot2`** chunks into one 'combi-chunk'. I prefer not to do this as I like saving the new dataframe I have created as an object in my environment before visualising it so that if anything has gone wrong (as things tend to do) I can more easily find the problem.\n\nRegardless of what one may or may not prefer to do, the one thing that must be mentioned about piping into **`ggplot2`** is that when we start with the `ggplot()` function we switch over from the pipe (`%>%`)\nto the plus sign (`+`). There are currently efforts to address this inconvenience, but are not yet ready for public consumption.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% # Choose starting dataframe\n  filter(site %in% c(\"Bordjies\", \"Tsitsikamma\", \"Humewood\", \"Durban\")) %>% # Select sites\n  select(-depth, -type) %>% # Remove depth and type columns\n  mutate(month = month(date), # Create month column\n         index = paste(site, src, sep = \"/ \")) %>% # Create individual site column\n  group_by(index, month) %>% # Group by individual sites and months\n  summarise(mean_temp = mean(temp, na.rm = TRUE), # Calculate mean temperature\n            sd_temp = sd(temp, na.rm = TRUE)) %>% # Calculate standard deviation\n  ggplot(aes(x = month, y = mean_temp)) + # Begin with ggplot, switch from '%>%' to '+'\n  geom_ribbon(aes(ymin = mean_temp - sd_temp, ymax = mean_temp + sd_temp), \n              fill = \"black\", alpha = 0.4) + # Create a ribbon\n  geom_line(col = \"red\", size = 0.3) + # Create lines within ribbon\n  facet_wrap(~index) + # Facet by individual sites\n  scale_x_continuous(breaks = seq(2, 12, 4)) + # Control x axis ticks\n  labs(x = \"Month\", y = \"Temperature (Â°C)\") + # Change labels\n  theme_dark() # Set theme\n```\n:::\n\n\n## Additional useful functions\n\nThere is an avalanche of useful functions to be found within the **`tidyverse`**. In truth, we have only looked at functions from three packages: **`ggplot2`**, **`dplyr`**, and **`tidyr`**. There are far, far too many functions even within these three packages to cover within a week. But that does not mean that the functions in other packages, such as **`purrr`** are not also massively useful for our work. More on that tomorrow. For now we will see how the inclusion of a handful of choice extra functions may help to make our workflow even tidier.\n\n### Rename variables (columns) with `rename()`\n\nWe have seen that we select columns in a dataframe with `select()`, but if we want to rename columns we have to use, you guessed it, `rename()`. This functions works by first telling R the new name you would like, and then the existing name of the column to be changed. This is perhaps a bit back to front, but such is life on occasion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  rename(source = src)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Create a new dataframe for a newly created variable (column) with `transmute()`\n\nIf for whatever reason one wanted to create a new variable (column), as one would do with `mutate()`, but one does not want to keep the dataframe from which the new column was created, the function to use is `transmute()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  transmute(kelvin = temp + 273.15)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nThis makes a bit more sense when paired with `group_by()` as it will pull over the grouping variables into the new dataframe. Note that when it does this for us automatically it will provide a message in the console.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  group_by(site, src) %>% \n  transmute(kelvin = temp + 273.15)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Count observations (rows) with `n()`\n\nWe have already seen this function sneak it's way into a few of the code chunks in the previous session. We use `n()` to count any grouped variable automatically. It is not able to be given any arguments, so we must organise our dataframe in order to satisfy it's needs. It is the diva function of the **`tidyverse`**; however, it is terribly useful as we usually want to know how many observations our summary stats are based. First we will run some stats and create a figure without documenting `n`. Then we will include `n` and see how that changes our conclusions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n SACTN_n <- SACTN %>% \n  group_by(site, src) %>% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %>% \n  arrange(mean_temp) %>% \n  ungroup() %>% \n  select(mean_temp) %>% \n  unique()\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point() +\n  labs(x = \"\", y = \"Temperature (Â°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n```\n:::\n\n\nThis looks like a pretty linear distribution of temperatures within the SACTN dataset. But now let's change the size of the dots to show how frequently each of these mean temperatures is occurring.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n SACTN_n <- SACTN %>% \n  group_by(site, src) %>% \n  summarise(mean_temp = round(mean(temp, na.rm = T))) %>% \n  ungroup() %>% \n  select(mean_temp) %>% \n  group_by(mean_temp) %>% \n  summarise(count = n())\n\nggplot(data = SACTN_n, aes(x = 1:nrow(SACTN_n), y = mean_temp)) +\n  geom_point(aes(size = count)) +\n  labs(x = \"\", y = \"Temperature (Â°C)\") +\n  theme(axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n```\n:::\n\n\nWe see now when we include the count (`n`) of the different mean temperatures that this distribution is not so even. There appear to be humps around 17Â°C and 22Â°C. Of course, we've created dot plots here just to illustrate this point. In reality if one were interested in a distribution like this one would use a histogram, or better yet, a density polygon.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  group_by(site, src) %>% \n  summarise(mean_temp = round(mean(temp, na.rm = T))\n            ) %>% \n  ungroup() %>% \n  ggplot(aes(x = mean_temp)) +\n  geom_density(fill = \"seagreen\", alpha = 0.6) +\n  labs(x = \"Temperature (Â°C)\")\n```\n:::\n\n\n\n### Select observations (rows) by number with `slice()`\n\nIf one wants to select only specific rows of a dataframe, rather than using some variable like we do for `filter()`, we use `slice()`. The function expects us to provide it with a series of integers as seen in the following code chunk. Try playing around with these values and see what happens\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Slice a seqeunce of rows\nSACTN %>% \n  slice(10010:10020)\n\n# Slice specific rows\nSACTN %>%\n  slice(c(1,8,19,24,3,400))\n\n# Slice all rows except these\nSACTN %>% \n  slice(-(c(1,8,4)))\n\n# Slice all rows except a sequence\nSACTN %>% \n  slice(-(1:1000))\n```\n:::\n\n\nIt is discouraged to use slice to remove or select specific rows of data as this does not discriminate against any possible future changes in ones data. Meaning that if at some point in the future new data are added to a dataset, re-running this code will likely no longer be selecting the correct rows. This is why `filter()` is a main function, and `slice()` is not. This auxiliary function can however still be quite useful when combined with arrange.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The top 5 variable sites as measured by SD\nSACTN %>% \n  group_by(site, src) %>% \n  summarise(sd_temp = sd(temp, na.rm = T)) %>% \n  ungroup() %>% \n  arrange(desc(sd_temp)) %>% \n  slice(1:5)\n```\n:::\n\n\n### Summary functions\n\nThere is a near endless sea of possibilities when one starts to become comfortable with writing R code. We have seen several summary functions used thus far. Mostly in straightforward ways. But that is one of the fun things about R, the only limits to what we may create are within our mind, not the program. Here is just one example of a creative way to answer a straightforward question: 'What is the proportion of recordings above 15Â°C per source?'. Note how we may refer to columns we have created within the same chunk. There is no need to save the intermediate dataframes if we choose not to.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nSACTN %>% \n  na.omit() %>% \n  group_by(src) %>%\n  summarise(count = n(), \n            count_15 = sum(temp > 15)) %>% \n  mutate(prop_15 = count_15/count) %>% \n  arrange(prop_15)\n```\n:::\n\n\n## The new age *redux*\n\nRemember the spreadsheet example from the first day of the R workshop? Here it is repeated in a more efficient way. Now with bonus ribbons! In this chunk we see how to load, transform, and visualise the data all in one go. One would not normally do this, but it sure is snappy!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv(\"data/SACTN_data.csv\") %>% # Load the SACTN Day 1 data\n  mutate(month = month(date)) %>% # Then create a month abbreviation column\n  group_by(site, month) %>% # Then group by sites and months\n  summarise(mean_temp = mean(temp, na.rm = TRUE), # Lastly calculate the mean\n            sd_temp = sd(temp, na.rm = TRUE)) %>% # and the SD\n  ggplot(aes(x = month, y = mean_temp)) + # Begin ggplot\n  geom_ribbon(aes(ymin = mean_temp - sd_temp, ymax = mean_temp + sd_temp), \n              fill = \"black\", alpha = 0.4) + # Create a ribbon\n  geom_point(aes(colour = site)) + # Create dots\n  geom_line(aes(colour = site, group = site)) + # Create lines\n  labs(x = \"\", y = \"Temperature (Â°C)\", colour = \"Site\") # Change labels\n```\n:::\n\n\n# Introduction\n\nNow that you've had some time to look at the data and work through the exercise in Excel, let's see how to do it in R. First we will want to load some packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\n```\n:::\n\n\nWith those packages loaded, R now knows the functions we will need to use to calculate the monthly climatologies for our three time series. We will also be able to make some pretty figures.\n\nUsing excel may allow one to make small changes quickly, but rapidly becomes laborious when any sophistication is required. The few lines of code below make all of the calculations we need in order to produce the results we are after.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nSACTN_data <- read_csv(\"data/SACTN_day_1.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create monthly climatologies\nSACTN_monthly <- SACTN_data %>% \n  mutate(month = month(date, label = T)) %>% \n  group_by(site, month) %>% \n  summarise(temp = mean(temp, na.rm = T))\n```\n:::\n\n\nAnd that is all there is to it. Load the data and use four short lines of code. But let's quickly make those bonus line plots while we're at it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = SACTN_monthly, aes(x = month, y = temp)) +\n  geom_point(aes(colour = site)) +\n  geom_line(aes(colour = site, group = site)) +\n  labs(x = \"\", y = \"Temperature (Â°C)\")\n```\n:::\n\n\nAnd if we want three different panels we just add one line of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = SACTN_monthly, aes(x = month, y = temp)) +\n  geom_point(aes(colour = site)) +\n  geom_line(aes(colour = site, group = site)) +\n  labs(x = \"\", y = \"Temperature (Â°C)\") +\n  facet_wrap(~site, ncol = 1) # Create panels\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}